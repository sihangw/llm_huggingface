#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Evaluator for mission→(objective, constraints, areas).

Usage example (from run_evaluation.py):
    from evaluator import Evaluator
    ev = Evaluator()
    score, details = ev.score_example(mission, pred, gold)

This module is self-contained (no external deps). It implements:
- Normalization (robust, idempotent) for objective/constraints/areas
- Validity & penalties (no "||", no placeholders, no invented numbers in constraints)
- Objective structural similarity (operator multiset F1) + wrapper agreement
- Constraints clause matching (greedy bipartite by similarity) + F1 + parameter/quantifier checks
- Areas Kendall-τ ranking score
- Surface tie-breaker (token F1 + normalized Levenshtein)
- Composite score with per-example weight renormalization

Behavioral ranking is optional (disabled by default here to keep things dependency-free and fast).

Author: generated by ChatGPT for Sihang Wei
"""
import re, json, math, random, unicodedata, argparse, sys, hashlib, statistics
from typing import List, Dict, Tuple, Any

# ------------------------ Utilities ------------------------

def nfkc(s: str) -> str:
    return unicodedata.normalize("NFKC", s)

def collapse_spaces(s: str) -> str:
    return re.sub(r'\s+', ' ', s).strip()

def strip_latex_spacing(s: str) -> str:
    return re.sub(r'\\(?:,|;|!|quad|qquad)\b', '', s)

def to_ascii_ops(s: str) -> str:
    s = s.replace('≤','<=').replace('≥','>=').replace('−','-')
    return s

def has_placeholders(s: str) -> bool:
    PLACEHOLDER_RE = re.compile(r'\bXXX\b|<(?:num|var)\b[^>]*>')
    return bool(PLACEHOLDER_RE.search(s))

def is_balanced(s: str) -> bool:
    # simple balance check for () and {}
    stack = []
    pairs = {')':'(', '}':'{'}
    for ch in s:
        if ch in '({':
            stack.append(ch)
        elif ch in ')}':
            if not stack or stack[-1] != pairs[ch]:
                return False
            stack.pop()
    return not stack

def one_shot_balance_repair(s: str) -> Tuple[str, bool]:
    # Attempt to fix a single missing or extra bracket
    opens = s.count('('); closes = s.count(')')
    repaired = False
    if opens == closes+1:
        s = s + ')'; repaired = True
    elif closes == opens+1 and s and s[-1] == ')':
        s = s[:-1]; repaired = True
    opens = s.count('{'); closes = s.count('}')
    if opens == closes+1:
        s = s + '}'; repaired = True
    elif closes == opens+1 and s and s[-1] == '}':
        s = s[:-1]; repaired = True
    return s, repaired

def tokenize_math(s: str) -> List[str]:
    # simple tokenizer for surface metrics
    return re.findall(r'[A-Za-z][A-Za-z0-9_*]*|\d+(?:\.\d+)?|[\+\-\*/(){}\[\],]|<=|>=|=|;|:|_|\\|P|\^', s)

def levenshtein_norm(a: str, b: str) -> float:
    # normalized Levenshtein similarity
    la, lb = len(a), len(b)
    if la == 0 and lb == 0: return 1.0
    # DP
    dp = list(range(lb+1))
    for i, ca in enumerate(a, start=1):
        prev = dp[0]
        dp[0] = i
        for j, cb in enumerate(b, start=1):
            cost = 0 if ca == cb else 1
            dp[j], prev = min(dp[j]+1, dp[j-1]+1, prev+cost), dp[j]
    dist = dp[lb]
    return 1.0 - dist / max(la, lb)

def jaccard(set_a: set, set_b: set) -> float:
    if not set_a and not set_b: return 1.0
    inter = len(set_a & set_b)
    union = len(set_a | set_b)
    return inter/union if union else 1.0

def kendall_tau_4(a: List[int], b: List[int]) -> float:
    # compute Kendall tau-like for permutations of 1..4 (fill missing by numeric order)
    def full_perm(lst):
        full = list(lst)
        for x in (1,2,3,4):
            if x not in full: full.append(x)
        return full[:4]
    pa, pb = full_perm(a), full_perm(b)
    pairs = [(i,j) for i in range(4) for j in range(i+1,4)]
    K = 0
    for (i,j) in pairs:
        ai, aj = pa[i], pa[j]
        bi, bj = pb[i], pb[j]
        if (ai - aj) * (bi - bj) < 0:
            K += 1
    return 1.0 - K/6.0

# Match a signed/unsigned float or int (optionally scientific notation),
# BUT only when it is NOT immediately preceded by a letter or underscore.
_NUM_RE = re.compile(
    r"(?<![A-Za-z_])"                 # not part of an identifier (e.g., x_1, norm2)
    r"[-+]?(?:\d+(?:\.\d+)?|\.\d+)"   # 123, 123.45, .5
    r"(?:[eE][+-]?\d+)?"
)

def numbers_in(s: str):
    """
    Return list of numeric literals as strings, excluding:
      - digits embedded in identifiers (norm2, x_1, t+1 where '1' is after '_')
      - exponents like '^2'
      - the exponent argument in pow(base, exponent)
    Keeps: 0.95 in CVaR_{0.95}, 10 in 'abs(x) <= 10', 0.1 in '0.1*abs(u)', etc.
    """
    out = []
    for m in _NUM_RE.finditer(s):
        i, j = m.span()
        val = m.group(0)

        # (1) Skip numbers that are immediately after a caret: '^2'
        k = i - 1
        while k >= 0 and s[k].isspace():
            k -= 1
        if k >= 0 and s[k] == '^':
            continue

        # (2) Skip numbers that appear as the exponent in pow(base, exponent)
        # Heuristic: look back a small window for the LAST 'pow(' with no ')' closing it first,
        # and ensure there's a comma after that 'pow(' before this number.
        lookback = s[max(0, i-120):j]  # small window is enough for short clauses
        p = lookback.rfind('pow(')
        if p != -1:
            # ensure there isn't a closing ')' between that 'pow(' and our number
            after_pow = lookback[p+4:]
            if ')' not in after_pow:
                # We are still inside that pow( ... ? ). If there's a comma, this number is likely exponent.
                if ',' in after_pow:
                    # Optionally: be stricter by ensuring THIS number is after the last comma.
                    if after_pow.rfind(',') < len(after_pow):
                        continue
        out.append(val)

    return out

# =========================
# Relation parsing helpers
# =========================
REL_FLIP = {"<": ">", ">": "<", "<=": ">=", ">=": "<=", "==": "=", "=": "="}

def split_top_level_rel(text: str) -> Tuple[str, str, str, int]:
    """Return (lhs, rel, rhs, idx) for first *top-level* relation; rel='' if none."""
    depth = 0
    i = 0
    while i < len(text):
        ch = text[i]
        if ch in "([{":
            depth += 1; i += 1; continue
        if ch in ")]}":
            depth = max(0, depth - 1); i += 1; continue
        if depth == 0:
            # two-char first
            for tok in ("<=", ">=", "=="):
                if text.startswith(tok, i):
                    lhs = text[:i].strip()
                    rhs = text[i+len(tok):].strip()
                    return lhs, ("=" if tok=="==" else tok), rhs, i
            # then one-char
            if ch in "<>=":
                lhs = text[:i].strip()
                rhs = text[i+1:].strip()
                return lhs, ch, rhs, i
        i += 1
    return text.strip(), "", "", -1

def find_balanced(text: str, start: int, open_ch: str, close_ch: str) -> int:
    """text[start] is an opener; return index just past its matching closer."""
    depth, i = 1, start + 1
    while i < len(text) and depth > 0:
        c = text[i]
        if c == open_ch: depth += 1
        elif c == close_ch: depth -= 1
        i += 1
    return i

# =========================
# Normalization (outer + nested)
# =========================
def normalize_orientation_recursive(text: str) -> str:
    """
    If exactly one numeric literal is on the LHS of a top-level relation and
    none on RHS, flip so the number moves to RHS. Recurse into NAME(...)/NAME[...].
    """
    lhs, rel, rhs, _ = split_top_level_rel(text)
    if rel:
        Lnums, Rnums = numbers_in(lhs), numbers_in(rhs)
        if len(Lnums) == 1 and len(Rnums) == 0:
            text = f"{rhs} {REL_FLIP.get(rel, rel)} {lhs}"
            lhs, rel, rhs, _ = split_top_level_rel(text)

    # recurse into NAME( ... ) / NAME[ ... ]
    i, last = 0, 0
    parts = []
    while i < len(text):
        if text[i].isalpha():
            j = i + 1
            while j < len(text) and (text[j].isalnum() or text[j] == '_'):
                j += 1
            if j < len(text) and text[j] in "([":
                open_ch = text[j]; close_ch = ")" if open_ch == "(" else "]"
                k = find_balanced(text, j, open_ch, close_ch)
                inner = text[j+1:k-1] if k-1 >= j+1 else ""
                inner_norm = normalize_orientation_recursive(inner)
                parts.append(text[last:j+1] + inner_norm + close_ch)
                i = k; last = i
                continue
        i += 1
    parts.append(text[last:])
    return "".join(parts)

# =========================
# Path-aware buckets
# =========================
def collect_numbers_with_paths(text: str, path: List[str] = None) -> Dict[Tuple[str, ...], List[float]]:
    """
    Map: PATH_TUPLE -> [numbers]. PATH components include operator names and
    role tags like 'REL:LHS(<=)', 'REL:RHS(>=)', 'INNER'.
    """
    if path is None: path = []
    buckets: Dict[Tuple[str, ...], List[float]] = {}

    def put(ns: List[str], role: str):
        if not ns: return
        key = tuple(path + [role])
        buckets.setdefault(key, []).extend(float(x) for x in ns)

    lhs, rel, rhs, _ = split_top_level_rel(text)
    if rel:
        put(numbers_in(lhs), f"REL:LHS({rel})")
        put(numbers_in(rhs), f"REL:RHS({rel})")
    else:
        put(numbers_in(text), "INNER")

    # recurse into NAME( ... ) / NAME[ ... ]
    i = 0
    while i < len(text):
        if text[i].isalpha():
            j = i + 1
            while j < len(text) and (text[j].isalnum() or text[j] == '_'):
                j += 1
            name = text[i:j]
            if j < len(text) and text[j] in "([":
                open_ch = text[j]; close_ch = ")" if open_ch == "(" else "]"
                k = find_balanced(text, j, open_ch, close_ch)
                inner = text[j+1:k-1] if k-1 >= j+1 else ""
                child = collect_numbers_with_paths(inner, path + [name])
                for pth, vals in child.items():
                    buckets.setdefault(pth, []).extend(vals)
                i = k; continue
        i += 1
    return buckets

def _truncate_paths(d: Dict[Tuple[str, ...], List[float]], max_depth: int) -> Dict[Tuple[str, ...], List[float]]:
    out: Dict[Tuple[str, ...], List[float]] = {}
    for pth, vals in d.items():
        tpth = pth[:max_depth]
        out.setdefault(tpth, []).extend(vals)
    return out

# =========================
# Role consistency + scoring
# =========================
def role_consistent_strict(pred_clause: str, gold_clause: str, max_depth: int = 3, eps: float = 1e-6) -> bool:
    """
    Normalize both clauses; require exact equality of truncated path sets,
    equal counts per path, and per-path values within 2% after sorted pairing.
    """
    pn = normalize_orientation_recursive(pred_clause)
    gn = normalize_orientation_recursive(gold_clause)
    Pb = _truncate_paths(collect_numbers_with_paths(pn), max_depth)
    Gb = _truncate_paths(collect_numbers_with_paths(gn), max_depth)

    if set(Pb.keys()) != set(Gb.keys()):
        return False
    for pth in Pb.keys():
        pv = sorted(Pb[pth]); gv = sorted(Gb[pth])
        if len(pv) != len(gv):
            return False
        for p, g in zip(pv, gv):
            tol = max(0.02*abs(g), eps)
            if abs(p - g) > tol:
                return False
    return True

def _per_path_numeric_score(pred_vals: List[float], gold_vals: List[float], eps: float = 1e-6) -> float:
    if not pred_vals and not gold_vals:
        return 1.0
    p = sorted(pred_vals); g = sorted(gold_vals)
    k = min(len(p), len(g))
    s = 0.0
    for i in range(k):
        tol = max(0.02*abs(g[i]), eps)
        diff = abs(p[i] - g[i])
        s += max(0.0, 1.0 - min(1.0, diff / tol))
    denom = max(len(p), len(g))  # leftovers count as 0
    return s / denom

def safe_param_score(pred_clause: str, gold_clause: str, max_depth: int = 3, eps: float = 1e-6) -> float:
    """
    1) Normalize orientation (outer + nested).
    2) If role-consistency (paths & side & operator) fails -> 0.0
    3) Else compute path-aware numeric closeness and average over all paths.
    """
    # role check
    if not role_consistent_strict(pred_clause, gold_clause, max_depth=max_depth, eps=eps):
        return 0.0

    # score per truncated path
    pn = normalize_orientation_recursive(pred_clause)
    gn = normalize_orientation_recursive(gold_clause)
    Pb = _truncate_paths(collect_numbers_with_paths(pn), max_depth)
    Gb = _truncate_paths(collect_numbers_with_paths(gn), max_depth)

    all_paths = set(Pb) | set(Gb)
    if not all_paths:
        return 1.0

    scores = []
    for pth in all_paths:
        scores.append(_per_path_numeric_score(Pb.get(pth, []), Gb.get(pth, []), eps=eps))
    return sum(scores) / len(scores)

# ------------------------ Normalizer ------------------------

class Normalizer:
    def __init__(self):
        # precompile patterns
        self.p_min  = re.compile(r'(?i)\\min\b|\\operatorname\{min\}\b|arg\s*min\b|minimize\b|minimization\b|minimum\b|\bmin\b')
        self.p_max  = re.compile(r'(?i)\\max\b|\\operatorname\{max\}\b|arg\s*max\b|maximize\b|maximization\b|maximum\b|\bmax\b')
        self.p_E    = re.compile(r'(?i)\\mathbb\{e\}|\bexpectation\b|\bexpect\b|\bmean\b|\bE\b')
        self.p_cvar = re.compile(r'(?i)\bcvar\b|conditional\s+value(?:-|\s+)at(?:-|\s+)risk')
        self.p_norm_inf = re.compile(r'\|\s*([^|]+?)\s*\|\s*_(?:inf|\\infty|∞)')
        self.p_norm_l1  = re.compile(r'\|\s*([^|]+?)\s*\|\s*_1\b')
        self.p_norm_l2  = re.compile(r'\|\s*([^|]+?)\\s*\|\s*(?:_2\b)?(?!\|)')
        self.p_abs      = re.compile(r'(?<!\|)\|\s*([^|]+?)\s*\|(?!\|)')
        self.p_pow_int  = re.compile(r'\(\s*([^()]+?)\s*\)\s*\^\s*([2-9]|[1-9]\d+)')
        self.p_diff2    = re.compile(r'Δ\^?2\s*([a-z]\w*(?:_\w+)?)')
        self.p_diff     = re.compile(r'Δ\s*([a-z]\w*(?:_\w+)?)')
        self.p_prob     = re.compile(r'(?i)(\\Pr|\\mathrm\{pr\}|\bprob(?:ability)?\b|\bP\b)\s*\(')
        self.p_target   = re.compile(r'x\^{\s*(?:ref|\*)\s*}|x\^\*|x_{ref}', re.IGNORECASE)

    def normalize_math(self, s: str) -> str:
        s = nfkc(s)
        s = to_ascii_ops(s)
        s = strip_latex_spacing(s)
        s = collapse_spaces(s)
        s, _ = one_shot_balance_repair(s)
        # lower except greek
        # (keep α ε ρ untouched by not lowercasing non-ascii letters)
        s = ''.join(ch.lower() if ord(ch) < 128 else ch for ch in s)
        # sentinels
        s = self.p_min.sub('__MIN__', s)
        s = self.p_max.sub('__MAX__', s)
        s = self.p_E.sub('__E__', s)
        s = self.p_cvar.sub('__CVAR__', s)
        # norms & abs
        s = self.p_norm_inf.sub(r'norminf(\1)', s)
        s = self.p_norm_l1.sub(r'norm1(\1)', s)
        # convert bare ||v|| or ||v||_2 (the pattern above with (?!\|) is fragile; do a safe pass)
        s = re.sub(r'\|\s*([^|]+?)\s*\|\s*(?:_2\b)?(?!\|)', r'norm2(\1)', s)
        s = self.p_abs.sub(r'abs(\1)', s)
        # powers & deltas
        for _ in range(5):
            s = self.p_pow_int.sub(r'pow(\1,\2)', s)
        s = self.p_diff2.sub(r'diff2(\1)', s)
        s = self.p_diff.sub(r'diff(\1)', s)
        # chance token
        s = self.p_prob.sub('P(', s)
        # wrappers
        s = self._build_wrappers(s)
        # variables & targets
        s = self.p_target.sub('x*', s)
        s = self._cleanup_spacing(s)
        return s

    def normalize_constraints(self, s: str) -> str:
        if not s: return ""
        s = nfkc(s); s = to_ascii_ops(s); s = strip_latex_spacing(s); s = collapse_spaces(s)
        s, _ = one_shot_balance_repair(s)
        s = ''.join(ch.lower() if ord(ch) < 128 else ch for ch in s)
        # inequalities
        s = s.replace('\\geq', '>=').replace('\\ge', '>=').replace('≥', '>=')
        s = s.replace('\\leq', '<=').replace('\\le', '<=').replace('≤', '<=')
        # chance
        s = self.p_prob.sub('P(', s)
        s = s.replace('\\epsilon', 'ε').replace('epsilon','ε')
        # norms/abs/pow/delta and wrappers
        s = self.normalize_math(s)
        # split/join by clauses for consistent separators
        clauses = re.split(r'(?:;|\\\\)+', s)
        clauses = [collapse_spaces(c) for c in clauses if collapse_spaces(c)]
        return '; '.join(clauses)

    def normalize_areas(self, A: List[int]) -> List[int]:
        if not isinstance(A, list): raise ValueError("areas not list")
        if len(set(A)) != len(A): raise ValueError("duplicate areas")
        if any(x not in (1,2,3,4) for x in A): raise ValueError("bad area label")
        return A[:]

    def _build_wrappers(self, s: str) -> str:
        # Replace sentinels with canonical wrappers with balanced bodies
        i = 0; out = []
        while i < len(s):
            if s.startswith('__MIN__', i):
                body, j = self._wrap_body(s, i+7)
                out.append(f'min{{{body}}}'); i = j; continue
            if s.startswith('__MAX__', i):
                body, j = self._wrap_body(s, i+7)
                out.append(f'max{{{body}}}'); i = j; continue
            if s.startswith('__E__', i):
                body, j = self._wrap_body(s, i+5, brackets='[]')
                out.append(f'E[{body}]'); i = j; continue
            if s.startswith('__CVAR__', i):
                alpha, k = self._consume_index(s, i+8)
                body, j = self._wrap_body(s, k, brackets='[]')
                alpha = alpha if alpha else 'α'
                out.append(f'CVaR_{{{alpha}}}[{body}]'); i = j; continue
            out.append(s[i]); i += 1
        return ''.join(out)

    def _consume_index(self, s: str, i: int) -> Tuple[str, int]:
        # consume _{...} or (alpha) if present; return (alpha_text, new_idx)
        i0 = i
        i = self._skip_ws(s, i)
        if i < len(s) and s[i] == '_':
            # _{...}
            i += 1; i = self._skip_ws(s, i)
            if i < len(s) and s[i] == '{':
                j = self._find_balanced(s, i)
                return s[i+1:j-1], j
        if i < len(s) and s[i] == '(':
            j = self._find_balanced(s, i)
            return s[i+1:j-1], j
        return '', i0

    def _wrap_body(self, s: str, i: int, brackets: str = '{}') -> Tuple[str, int]:
        i = self._skip_ws(s, i)
        if i < len(s) and s[i] in '([{':
            j = self._find_balanced(s, i)
            body = s[i+1:j-1]
            return body, j
        # else read until top-level ; or \\ or end
        depth = 0; j = i
        while j < len(s):
            ch = s[j]
            if ch in '({[': depth += 1
            elif ch in ')}]': depth = max(0, depth-1)
            if depth == 0:
                if s.startswith('\\\\', j) or ch == ';':
                    break
            j += 1
        body = s[i:j].strip()
        if not body: body = '0'
        return body, j

    def _find_balanced(self, s: str, i: int) -> int:
        stack = [s[i]]
        j = i+1
        pairs = {')':'(', '}':'{', ']':'['}
        while j < len(s) and stack:
            ch = s[j]
            if ch in '([{':
                stack.append(ch)
            elif ch in ')]}':
                if not stack or pairs[ch] != stack[-1]:
                    break
                stack.pop()
            j += 1
        return j

    def _skip_ws(self, s: str, i: int) -> int:
        while i < len(s) and s[i].isspace(): i += 1
        return i

    def _cleanup_spacing(self, s: str) -> str:
        s = re.sub(r'\s+', ' ', s).strip()
        s = re.sub(r'\s*([\(\)\{\}\[\]\+\-\*/=;,])\s*', r'\1', s)
        s = re.sub(r'([^\d])\s+([^\d])', r'\1 \2', s)
        return s

# ------------------------ Scoring ------------------------

OP_TYPES = ['PLUS','MINUS','TIMES','DIV','POW','ABS','LOG','INV','HINGE',
            'E','CVAR','MIN','MAX','SUM_T','NORM1','NORM2','NORMINF']

def operator_multiset(s: str) -> Dict[str,int]:
    # Count occurrences of canonical operators in a normalized objective string
    counts = {k:0 for k in OP_TYPES}
    # arithmetic
    counts['PLUS'] += s.count('+')
    counts['MINUS'] += s.count('-')
    counts['TIMES'] += s.count('*')
    counts['DIV'] += s.count('/')
    counts['POW'] += len(re.findall(r'\bpow\(', s))
    counts['ABS'] += len(re.findall(r'\babs\(', s))
    counts['LOG'] += len(re.findall(r'\blog\(', s))
    counts['INV'] += len(re.findall(r'\binv\(', s))
    counts['HINGE'] += len(re.findall(r'\bhinge\(', s))
    counts['E'] += len(re.findall(r'\bE\[(?:.*?)\]', s))
    counts['CVAR'] += len(re.findall(r'\bCVaR_\{', s))
    counts['MIN'] += len(re.findall(r'\bmin\{', s))
    counts['MAX'] += len(re.findall(r'\bmax\{', s))
    counts['SUM_T'] += len(re.findall(r'\bsum_t\{', s))
    counts['NORM1'] += len(re.findall(r'\bnorm1\(', s))
    counts['NORM2'] += len(re.findall(r'\bnorm2\(', s))
    counts['NORMINF'] += len(re.findall(r'\bnorminf\(', s))
    return counts

def multiset_f1(c1: Dict[str,int], c2: Dict[str,int]) -> float:
    tp = sum(min(c1[k], c2[k]) for k in c1.keys())
    fp = sum(max(0, c1[k]-c2[k]) for k in c1.keys())
    fn = sum(max(0, c2[k]-c1[k]) for k in c1.keys())
    denom = 2*tp + fp + fn
    return (2*tp/denom) if denom > 0 else 1.0

def wrapper_of(s: str) -> Tuple[str, str]:
    # returns (wrapper_type, param) where type ∈ {'none','min','max','E','CVaR'}
    if s.startswith('min{'): return ('min','')
    if s.startswith('max{'): return ('max','')
    m = re.match(r'E\[(.*)\]$', s)
    if m: return ('E','')
    m = re.match(r'CVaR_\{([^}]+)\}\[(.*)\]$', s)
    if m: return ('CVaR', m.group(1))
    return ('none','')

def token_f1(a: str, b: str) -> float:
    ta, tb = tokenize_math(a), tokenize_math(b)
    if not ta and not tb: return 1.0
    # multiset token F1
    from collections import Counter
    ca, cb = Counter(ta), Counter(tb)
    tp = sum((ca & cb).values())
    fp = sum((ca - cb).values())
    fn = sum((cb - ca).values())
    denom = 2*tp + fp + fn
    return (2*tp/denom) if denom>0 else 1.0

def clause_split(s: str) -> List[str]:
    if not s: return []
    parts = [x.strip() for x in re.split(r'(?:;|\\\\)+', s)]
    return [p for p in parts if p]

def clause_signature(c: str) -> Dict[str, Any]:
    # build a lightweight signature for matching
    rel = '<=' if '<=' in c else '>=' if '>=' in c else '=' if '=' in c else ''
    vars_ = set(re.findall(r'\b[a-z][a-z0-9_]*\b', c))
    ops = set()
    for t in ['abs(', 'pow(', 'log(', 'inv(', 'hinge(', 'norm1(', 'norm2(', 'norminf(', 'sum_t{', 'E[', 'CVaR_{', 'min{', 'max{', 'P(']:
        if t in c: ops.add(t.rstrip('({['))
    ctype = 'chance' if 'P(' in c else 'eq' if rel=='=' else 'ineq'
    quant_allt = 'forall_t' if re.search(r'\bfor\s+all\s+t\b', c) else ''
    nums = re.findall(r'\d+(?:\.\d+)?', c)
    eps = re.findall(r'1-ε', c)
    return {'rel': rel, 'vars': vars_, 'ops': ops, 'ctype': ctype, 'quant': quant_allt, 'nums': nums, 'has_eps': bool(eps)}

def clause_similarity(c1: str, c2: str) -> float:
    s1, s2 = clause_signature(c1), clause_signature(c2)
    j_vars = jaccard(s1['vars'], s2['vars'])
    j_ops  = jaccard(s1['ops'], s2['ops'])
    bonus_rel = 1.0 if s1['rel'] == s2['rel'] else 0.0
    bonus_type= 1.0 if s1['ctype'] == s2['ctype'] else 0.0
    sim = 0.5*j_vars + 0.2*j_ops + 0.15*bonus_rel + 0.15*bonus_type
    return sim

def greedy_match(p_list: List[str], g_list: List[str], thr: float=0.5) -> List[Tuple[int,int,float]]:
    pairs = []
    scored = []
    for i, p in enumerate(p_list):
        for j, g in enumerate(g_list):
            s = clause_similarity(p, g)
            scored.append((s,i,j))
    scored.sort(reverse=True)
    used_i, used_j = set(), set()
    for s,i,j in scored:
        if s < thr: break
        if i in used_i or j in used_j: continue
        pairs.append((i,j,s))
        used_i.add(i); used_j.add(j)
    return pairs

class Evaluator:
    def __init__(self, enable_behavioral: bool=False):
        self.N = Normalizer()
        self.enable_behavioral = enable_behavioral  # placeholder; behavioral not implemented in this minimal version

        # weights
        self.w_obj = 0.45
        self.w_con = 0.35
        self.w_area = 0.15
        self.w_surf = 0.05

    # ---------- Public API ----------
    def score_example(self, mission: str, pred: Dict[str,Any], gold: Dict[str,Any]) -> Tuple[float, Dict[str,Any]]:
        # Normalize
        obj_p = self.N.normalize_math(pred['objective_function'])
        con_p = self.N.normalize_constraints(pred['constraints'] or '')
        try:
            areas_p = self.N.normalize_areas(pred['areas'])
        except Exception:
            areas_p = []

        obj_g = self.N.normalize_math(gold['objective_function'])
        con_g = self.N.normalize_constraints(gold['constraints'] or '')
        areas_g = self.N.normalize_areas(gold['areas'])

        # Validity
        validity, reasons = self._validity(mission, obj_p, con_p, areas_p, obj_g, con_g)
        if not validity:
            return 0.0, {'validity':0, 'reasons':reasons}

        # Objective subscores
        struct = multiset_f1(operator_multiset(obj_p), operator_multiset(obj_g))
        wtype_p, alpha_p = wrapper_of(obj_p)
        wtype_g, alpha_g = wrapper_of(obj_g)
        wrapper = 0.0
        if wtype_p == wtype_g:
            if wtype_p == 'CVaR':
                # alpha match: numeric vs numeric or both symbolic
                try:
                    ap = float(alpha_p)
                    ag = float(alpha_g)
                    wrapper = 1.0 if abs(ap-ag) <= 0.01 else 0.0
                except:
                    wrapper = 1.0  # treat both symbolic as match
            else:
                wrapper = 1.0

        # Behavioral placeholder (set to 0.5 neutral if disabled)
        behavioral = 0.5

        S_obj = 0.60*struct + 0.25*wrapper + 0.15*behavioral

        # Constraints subscores
        if con_p=="" and con_g=="":
            S_con = 1.0
            clauseF1 = 1.0; param_acc = 1.0; quant = 1.0
        elif (con_p=="" and con_g!="") or (con_p!="" and con_g==""):
            S_con = 0.0
            clauseF1 = 0.0; param_acc = 0.0; quant = 0.0
        else:
            P = clause_split(con_p); G = clause_split(con_g)
            pairs = greedy_match(P, G, thr=0.5)
            tp = len(pairs); fp = len(P)-tp; fn = len(G)-tp
            clauseF1 = (2*tp)/(2*tp+fp+fn) if (2*tp+fp+fn)>0 else 1.0
            # ParamAcc: check numbers & inequality
            num_scores = []
            rel_scores = []
            quant_scores = []
            for i,j,s in pairs:
                pi, gi = P[i], G[j]
                num_scores.append(safe_param_score(pi, gi))  # returns [0,1]
                rel_p = '<=' if '<=' in pi else '>=' if '>=' in pi else '=' if '=' in pi else ''
                rel_g = '<=' if '<=' in gi else '>=' if '>=' in gi else '=' if '=' in gi else ''
                rel_scores.append(1.0 if rel_p == rel_g else 0.0)
                # Quant (chance + ∀t)
                chance_p = 'P(' in pi; chance_g = 'P(' in gi
                eps_p = '1-ε' in pi; eps_g = '1-ε' in gi
                q_score = 1.0 if chance_p==chance_g else 0.0
                q_score = q_score * (1.0 if eps_p==eps_g else 0.9 if (chance_p and chance_g) else q_score)
                allt_p = bool(re.search(r'\bfor all t\b', pi)); allt_g = bool(re.search(r'\bfor all t\b', gi))
                q_score = 0.75*q_score + 0.25*(1.0 if allt_p==allt_g else 0.0)
                quant_scores.append(q_score)
            param_acc = sum(num_scores)/len(num_scores) if num_scores else 1.0
            quant = sum(quant_scores)/len(quant_scores) if quant_scores else 1.0
            S_con = 0.45*clauseF1 + 0.30*param_acc + 0.25*quant

        # Areas
        S_area = kendall_tau_4(areas_p, areas_g) if areas_p else 0.0

        # Surface
        surf = 0.5*token_f1(obj_p + ' | ' + con_p, obj_g + ' | ' + con_g) + 0.5*levenshtein_norm(obj_p + ';' + con_p, obj_g + ';' + con_g)

        # Combine with weight renormalization to available blocks
        parts = []
        weights = []
        parts.append(S_obj); weights.append(self.w_obj)
        parts.append(S_con); weights.append(self.w_con)
        parts.append(S_area); weights.append(self.w_area)
        parts.append(surf); weights.append(self.w_surf)

        W = sum(weights)
        combo = sum(w*p for w,p in zip(weights, parts)) / (W if W>0 else 1.0)
        final = 1.0 * combo  # Validity already enforced

        details = {
            'validity': 1,
            'S_obj': S_obj, 'struct': struct, 'wrapper': wrapper, 'behavioral': behavioral,
            'S_con': S_con, 'clauseF1': clauseF1, 'param_acc': param_acc, 'quant': quant,
            'S_area': S_area, 'S_surf': surf,
            'obj_norm_pred': obj_p, 'obj_norm_gold': obj_g,
            'con_norm_pred': con_p, 'con_norm_gold': con_g,
        }
        return final, details

    # ---------- Internals ----------
    def _validity(self, mission: str, obj_p: str, con_p: str, areas_p: List[int], obj_g: str, con_g: str) -> Tuple[bool, List[str]]:
        reasons = []
        if not obj_p or '\n' in obj_p:
            reasons.append('OBJ_EMPTY_OR_MULTILINE')
        sp = obj_p + ' ' + con_p

        if '||' in sp:
            reasons.append('BANNED_BARS')
        if has_placeholders(sp):
            reasons.append('PLACEHOLDERS')
        if not is_balanced(obj_p): reasons.append('OBJ_UNBALANCED')
        if con_p and not is_balanced(con_p): reasons.append('CONS_UNBALANCED')
        # areas
        if not isinstance(areas_p, list) or any(x not in (1,2,3,4) for x in areas_p) or len(set(areas_p))!=len(areas_p):
            reasons.append('AREAS_INVALID')
        # numbers in constraints: must be subset of mission or gold or {0,1}
        nums_p = set(numbers_in(con_p))
        nums_ok = set(numbers_in(con_g)) | set(numbers_in(mission)) | {'0','1'}
        if not nums_p.issubset(nums_ok):
            reasons.append('INVENT_NUM_IN_CONS')
        return (len(reasons)==0), reasons